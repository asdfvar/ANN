#include "ann.h"
#include <stdlib.h>
#include <time.h>
#include <math.h>

#if 0
   ann::ann(int *LS, int n){

      ann::init(int *LS, int n, -1.0f, 4.0f, 0.25f);

      LayerSizes = LS;  // Number of nodes per layer
      N    = n;         // Size of LayerSizes array
      bias = -1.0f;     // Bias term
      beta = 4.0f;      // Beta term used in the sigmoid function
      nu   = 0.25f;     // Learning rate
      
      int i,j,NN;
      
      srand (time(NULL));
      
      // Build the weights
      NN = 0;
      for (j = 0; j < N-1; j++)
         NN += LayerSizes[j]*LayerSizes[j+1];
      
      w = (float*) calloc(NN, sizeof(float));
      
      for (j = 0; j < NN; j++)
         w[j] = ((float) rand()) / ((float) RAND_MAX) * 2.0f - 1.0f;
      
      // Build the bias weights
      NN = 0;
      for (j = 1; j < N; j++)
         NN += LayerSizes[j];
      
      v = (float*) calloc(NN, sizeof(float));
      
      for (j = 0; j < NN; j++)
         v[j] = ((float) rand()) / ((float) RAND_MAX) * 2.0f - 1.0f;
      
      // Build the layers
      NN = 0;
      for (j = 0; j < N; j++)
         NN += LayerSizes[j];
      
      x = (float*) calloc(NN, sizeof(float));
      
      // Build the back propagation error arrays
      NN = 0;
      for (j = 0; j < N; j++)
         NN += LayerSizes[j];
      
      d = (float*) calloc(NN, sizeof(float));
   }
#endif
   
   ann::ann(int *LS, int n, float bs, float bta, float Nu){
      LayerSizes = LS;  // Number of nodes per layer
      N = n;            // Size of LayerSizes array
      bias = bs;        // Bias term
      beta = bta;       // Beta term used in the sigmoid function
      nu = Nu;          // Learning rate
      
      int i,j,NN;
      
      srand (time(NULL));
      
      // Build the weights
      NN = 0;
      for (j = 0; j < N-1; j++)
         NN += LayerSizes[j]*LayerSizes[j+1];
      
      w = (float*) calloc(NN, sizeof(float));
      
      for (j = 0; j < NN; j++)
         w[j] = ((float) rand()) / ((float) RAND_MAX) * 2.0f - 1.0f;

      // Build the bias weights
      NN = 0;
      for (j = 1; j < N; j++)
         NN += LayerSizes[j];
      
      v = (float*) calloc(NN, sizeof(float));
      
      for (j = 0; j < NN; j++)
         v[j] = ((float) rand()) / ((float) RAND_MAX) * 2.0f - 1.0f;
      
      // Build the layers
      NN = 0;
      for (j = 0; j < N; j++)
         NN += LayerSizes[j];
      
      x = (float*) calloc(NN, sizeof(float));
      
      // Build the back propagation error arrays
      NN = 0;
      for (j = 0; j < N; j++)
         NN += LayerSizes[j];
      
      d = (float*) calloc(NN, sizeof(float));

   }
   
   ann::~ann(){}

/*************************************************************************************************/
#if 0
void ann::init(int *LS, int n, float bs, float bta, float Nu){

      LayerSizes = LS;  // Number of nodes per layer
      N = n;            // Size of LayerSizes array
      bias = bs;        // Bias term
      beta = bta;       // Beta term used in the sigmoid function
      nu = Nu;          // Learning rate
      
      int i,j,NN;
      
      srand (time(NULL));
      
      // Build the weights
      NN = 0;
      for (j = 0; j < N-1; j++)
         NN += LayerSizes[j]*LayerSizes[j+1];
      
      w = (float*) calloc(NN, sizeof(float));
      
      for (j = 0; j < NN; j++)
         w[j] = ((float) rand()) / ((float) RAND_MAX) * 2.0f - 1.0f;

      // Build the bias weights
      NN = 0;
      for (j = 1; j < N; j++)
         NN += LayerSizes[j];
      
      v = (float*) calloc(NN, sizeof(float));
      
      for (j = 0; j < NN; j++)
         v[j] = ((float) rand()) / ((float) RAND_MAX) * 2.0f - 1.0f;
      
      // Build the layers
      NN = 0;
      for (j = 0; j < N; j++)
         NN += LayerSizes[j];
      
      x = (float*) calloc(NN, sizeof(float));
      
      // Build the back propagation error arrays
      NN = 0;
      for (j = 0; j < N; j++)
         NN += LayerSizes[j];
      
      d = (float*) calloc(NN, sizeof(float));

}
#endif
/*************************************************************************************************/
   
   /******************************************************
    * Feed forward
    *
    * Return a pointer to the output of the network given
    * the provided input.
    ******************************************************/
#if 0
    float *ann::forward(float *inp){
       
       int i,j;
       float *xp, *wp, *vp;
       float *output;
       
       // Set the first layer values to the input values
       for (i = 0; i < LayerSizes[0]; i++)
          x[i] = inp[i];
       
       // Define the current positions
       xp = x; wp = w; vp = v;
                  
       for (i = 0; i < N-1; i++){
       
          // Apply the weights to the next layer
          mult(xp + LayerSizes[i], wp, xp, LayerSizes[i+1], LayerSizes[i]);
          
          // Advance to the next layer
          xp += LayerSizes[i];
          
          // Advance the weights to the next layer
          wp += LayerSizes[i] * LayerSizes[i+1];
          
          // Apply the bias weights to the current layer
          for (j = 0; j < LayerSizes[i+1]; j++)
             xp[j] += bias * vp[j];
          
          // Advance the bias weights to the next layer
          vp += LayerSizes[i+1];
          
          // Apply the activation function to the current layer
          for (j = 0; j < LayerSizes[i+1]; j++)
             xp[j] = g(xp[j], beta);
          
       }
       
       output = xp;
       
       return output;
    }
#endif
#if 0
  /*****************************************************
   * Training
   *
   * Train the network with the provided input and output
   * data sets using the back propagation algorithm over
   * the user supplied number of steps.
   *
   * There are K input and output sets.
   *****************************************************/
   
   void ann::train(float *inpSet, float *outSet, int K, int Steps){
      
      int i,j,k;
      int Set;            // Selected set
      float *inp, *out;   // Desired output with provided input
      float *y;           // Network Layer output
      float *d0;          // Output error
      float *di;          // Interior error
      float *wp;          // Weights
      float *vp;          // Bias weights
      float *xp;          // Layer
      int Max = 0;
      int tmp;
      
      for (i = 0; i < N-1; i++){
         tmp = LayerSizes[i]*LayerSizes[i+1];
         Max = (tmp > Max) ? tmp : Max;
      }
      
      float buf[Max];    // Buffer space
      
      float *dw, *dv;
      
      float Error;
      
      // Define the output error position
      d0 = d;
      for (k = 0; k < N-1; k++)
         d0 += LayerSizes[k];
      
      for (k = 0; k < Steps; k++){
      
         // Randomly select a training set
         Set = rand() % K;
         inp = inpSet + Set * LayerSizes[0];
         out = outSet + Set * LayerSizes[N-1];
         
         // Feed forward
         y = forward(inp);
         
         // Get the output error
         for (i = 0; i < LayerSizes[N-1]; i++)
            d0[i] = out[i] - y[i];
         
         // Define the interior layer position
         xp = x;
         for (i = 0; i < N-2; i++)
            xp += LayerSizes[i];
         
         // Define the interior error position
         di = d0 - LayerSizes[N-2];
      
         // Define the weights position
         wp = w;
         for (i = 0; i < N-2; i++)
            wp += LayerSizes[i] * LayerSizes[i+1];
         
         // Back propagate the error
         for (i = N-2; i > 0; i--){
         
            // Get the current layer error
            Tmult(&buf[0], wp, di + LayerSizes[i], LayerSizes[i+1], LayerSizes[i]);
            for (j = 0; j < LayerSizes[i]; j++)
               di[j] = buf[j] * beta * xp[j] * (1.0f - xp[j]);
            
            if (i > 0){ // maybe not needed
               // Move the interior layer position back one layer
               xp -= LayerSizes[i-1];
               
               // Move the interior error position back one layer
               di -= LayerSizes[i-1];
               
               // Move the weights position back one layer
               wp -= LayerSizes[i] * LayerSizes[i-1];
            }
            
         }
         
         di = d + LayerSizes[0];
         xp = x;
         wp = w;
         vp = v;
         
         // Update the weights
         for (i = 0; i < N-1; i++){
         
            // Get the weight changes
            dw = &buf[0];
            outer(dw, di, xp, LayerSizes[i+1], LayerSizes[i]);
            
            // Update the weights
            for (j = 0; j < LayerSizes[i] * LayerSizes[i+1]; j++)
               wp[j] += nu * dw[j];

            // Get the bias weight changes
            dv = &buf[0];
            for (j = 0; j < LayerSizes[i+1]; j++)
               dv[j] = di[j] * bias;
            
            // Update the bias weights
            for (j = 0; j < LayerSizes[i+1]; j++)
               vp[j] += nu * dv[j];
            
            // Move forward a layer
            di += LayerSizes[i+1];
            xp += LayerSizes[i];
            wp += LayerSizes[i] * LayerSizes[i+1];
            vp += LayerSizes[i+1];
         }

         Error = 0.0f;
         for (i = 0; i < LayerSizes[N-1]; i++)
            Error += d0[i]*d0[i];
         printf("Error = %f\n", Error);
      }
   }
#endif
  /*****************************************************
   * Sigmoid function
   *
   *                 1
   * g(z) =  --------------------
   *          1 + exp(-beta * z)
   *
   *****************************************************/
   
   float ann::g(float z, float beta){
      return 1.0f / (1.0f + exp(-beta * z));
   }
   
   /**************************************************
    * Apply an MxN matrix to the N dimensional vector
    * b
    *
    * y = A*b
    **************************************************/
   
   void ann::mult(float *y, float *A, float *b, int M, int N){
   
      int i,j,ind;
      
      for (i = 0, ind = 0; i < M; i++){
         y[i] = 0;
         for (j = 0; j < N; j++, ind++)
            y[i] += A[ind] * b[j];
      }
   }
   
  /****************************************************
   * Apply the transpose of an MxN matrix to the
   * M dimensional vector b
   *
   * y = A'*b
   ****************************************************/
   
   void ann::Tmult(float *y, float *A, float *b, int M, int N){
   
      float At[N][M];
      int i,j,ind;
      
      // Transpose A
      for (i = 0, ind = 0; i < M; i++)
         for (j = 0; j < N; j++, ind++)
            At[j][i] = A[ind];
      
      mult(y, &At[0][0], b, N, M);
   }
   
  /**************************************************
   * Apply the outer product of vector x of length M
   * and vector y of length N and put the result into
   * vector z.
   **************************************************/
   
   void ann::outer(float *z, float *x, float *y, int M, int N){
   
      int i,j,ind;
      
      for (i = 0, ind = 0; i < M; i++)
         for (j = 0; j < N; j++, ind++)
            z[ind] = x[i]*y[j];
   
   }
